from langchain_groq import ChatGroq
from langchain_core.messages import SystemMessage, HumanMessage
import yaml
import os
from utils.logger import logger
from utils.error_handler import safe_execute

# Load config
with open("config/config.yaml", "r") as f:
    config = yaml.safe_load(f)

class EvaluatorAgent:
    def __init__(self):
        logger.info("Initializing EvaluatorAgent")
        self.llm = ChatGroq(
            model=config["llm"]["model"],
            temperature=0,
            groq_api_key=os.getenv("GROQ_API_KEY")
        )

    @safe_execute(default_return="Error: Evaluation failed.", log_context="EvaluatorAgent.evaluate", retries=3)
    def evaluate(self, query: str, final_report: str) -> str:
        """
        Reviews the final report to ensure it answers the user query and is grounded in data.
        """
        logger.info("Evaluating final report...")
        system_prompt = """You are the Evaluator Agent.
Your job is to quality-check the final report generated by the system.

Checklist:
1. Does the report directly answer the user's query?
2. Are the insights supported by data (numbers/metrics)?
3. Are the creative recommendations relevant?

If the report is good, output "PASS".
If there are issues, output "FAIL: <reason>" and suggestions for improvement.
"""
        response = self.llm.invoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"User Query: {query}\n\nFinal Report:\n{final_report}")
        ])
        return response.content
