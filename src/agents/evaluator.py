from langchain_groq import ChatGroq
from langchain_core.messages import SystemMessage, HumanMessage
import yaml
import os
import json
from src.utils.logger import logger
from src.utils.error_handler import safe_execute

# Load config
with open("config/config.yaml", "r") as f:
    config = yaml.safe_load(f)

class EvaluatorAgent:
    def __init__(self):
        logger.info("Initializing EvaluatorAgent")
        self.llm = ChatGroq(
            model=config["llm"]["model"],
            temperature=0,
            groq_api_key=os.getenv("GROQ_API_KEY")
        )

    def validate_statistical_rigor(self, insights_json: str) -> list:
        """
        Programmatic check for statistical rigor in insights.
        Returns a list of validation errors.
        """
        errors = []
        try:
            insights = json.loads(insights_json)
            for i, insight in enumerate(insights):
                # Check confidence
                if "confidence" not in insight or not isinstance(insight["confidence"], (int, float)):
                    errors.append(f"Insight {i+1}: Missing or invalid confidence score.")
                elif insight["confidence"] < 0 or insight["confidence"] > 1:
                    errors.append(f"Insight {i+1}: Confidence score {insight['confidence']} out of range (0-1).")
                
                # Check evidence
                if "evidence" not in insight or not insight["evidence"]:
                    errors.append(f"Insight {i+1}: No evidence provided.")
                else:
                    for ev in insight["evidence"]:
                        if "delta" not in ev or not ev["delta"]:
                            errors.append(f"Insight {i+1}: Evidence missing 'delta' value.")
                        if "metric" not in ev or not ev["metric"]:
                            errors.append(f"Insight {i+1}: Evidence missing 'metric' name.")
                            
        except json.JSONDecodeError:
            errors.append("Insights are not valid JSON.")
        except Exception as e:
            errors.append(f"Validation error: {str(e)}")
            
        return errors

    @safe_execute(default_return="Error: Evaluation failed.", log_context="EvaluatorAgent.evaluate", retries=3)
    def evaluate(self, query: str, final_report: str, insights_json: str) -> str:
        """
        Reviews the final report and performs statistical validation.
        """
        logger.info("Evaluating final report...")
        
        # 1. Statistical Validation (Code-based)
        stat_errors = self.validate_statistical_rigor(insights_json)
        stat_validation_msg = "PASS" if not stat_errors else f"FAIL ({len(stat_errors)} issues found)"
        
        if stat_errors:
            logger.warning(f"Statistical Validation Failed: {stat_errors}")
        else:
            logger.info("Statistical Validation Passed.")

        # 2. Qualitative Validation (LLM-based)
        system_prompt = """You are the Evaluator Agent.
Your job is to quality-check the final report generated by the system.

Checklist:
1. Does the report directly answer the user's query?
2. Are the insights supported by data (numbers/metrics)?
3. Are the creative recommendations relevant?

Input:
- User Query
- Final Report
- Statistical Validation Result (from code)

Output:
If the report is good AND Statistical Validation Passed: output "PASS".
If there are issues, output "FAIL: <reason>" and suggestions for improvement.
"""
        response = self.llm.invoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"User Query: {query}\n\nStatistical Validation: {stat_validation_msg}\nErrors: {stat_errors}\n\nFinal Report:\n{final_report}")
        ])
        
        result = response.content
        logger.decision("EvaluatorAgent", query, result, "Evaluated report quality")
        
        return result
